# -*- coding: utf-8 -*-
"""pythonFinalProjectPrabina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CGl6D7hLNpDwXrTg-jdSKTfth7Is-pgT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
import seaborn as sns

url="https://raw.githubusercontent.com/mohammedAljadd/students-performance-prediction/refs/heads/main/student-data.csv"
df=pd.read_csv(url)

df.head()

df.shape

df.isnull().sum() #any values null

boolean_columns = ['schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']

df[boolean_columns] = df[boolean_columns].replace({'yes': 1, 'no': 0})

features = df[['failures', 'health', 'age', 'studytime', 'absences', 'goout', 'freetime'] + boolean_columns]
target=df['passed']

X=features.values
y=target.values

X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)

y_train = pd.Series(y_train)
y_test = pd.Series(y_test)
y_train = y_train.map({'yes': 1, 'no': 0})
y_test = y_test.map({'yes': 1, 'no': 0})

scaler=StandardScaler() #to fix weight of multiple features and make it in same range
# normalizing values of x_train and x_test
X_train = scaler.fit_transform(X_train) #normalizing -->look for details
X_test=scaler.transform(X_test)

X_train=torch.tensor(X_train, dtype=torch.float32)
X_test=torch.tensor(X_test, dtype=torch.float32)

y_train=torch.tensor(y_train, dtype = int)
y_test=torch.tensor(y_test, dtype = int)

class MultiClassClassifier(nn.Module):
    def __init__(self):
        super(MultiClassClassifier, self).__init__()
        self.Linear1 = nn.Linear(15, 256)
        self.Linear2 = nn.Linear(256,128)
        self.Linear3 = nn.Linear(128, 3)

    def forward(self, x):
        x1 = torch.relu(self.Linear1(x))
        x2 = torch.relu(self.Linear2(x1))
        x3 = torch.relu(self.Linear3(x2))
        x4 = torch.softmax(x3, dim=1)
        return x3

model = MultiClassClassifier()
loss = nn.CrossEntropyLoss()
criteria = torch.optim.SGD(model.parameters(), lr = 0.05)
num_epochs=1000 #number of times

# creating lists to store etst and train loss
train_loss=[]
test_loss=[]

# training loop
for ep in range(num_epochs):
  model.train()
  predicted_y = model(X_train)
  losses = loss(predicted_y, y_train)

  #feedback
  criteria.zero_grad()
  losses.backward()
  criteria.step()
  if ep % 100 == 0:
    print(f"Epoch {ep}: Train Loss = {losses.item()}")

  train_loss.append(losses.item())
  model.eval()
  with torch.no_grad():
    predicted_test_y=model(X_test)
    loss_test=loss(predicted_test_y,y_test)
    test_loss.append(loss_test.item())
    if ep % 100 == 0:
      print(f"Epoch {ep}: Test Loss = {loss_test.item()}")

plt.plot(test_loss, label = 'Test Loss')
plt.plot(train_loss, color='red', label = 'Train Loss')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, predicted_test_y.argmax(dim=1))  # Use .argmax to get the predicted class indices
print(cm)
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g',
            xticklabels='auto', yticklabels='auto')
plt.show()



from sklearn.metrics import accuracy_score

with torch.no_grad():
    # Get the predicted probabilities
    predicted = model(X_test)

    # Convert the probabilities to predicted class indices
    predicted_classes = predicted.argmax(dim=1)  # Get the index of the highest probability class

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predicted_classes)  # Compare with true labels
    print(f'Accuracy : {accuracy*100:.4f} %')